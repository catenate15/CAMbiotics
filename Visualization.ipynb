import torch
import numpy as np
from torch.autograd import Function
import torch.nn.functional as F
from model import ConvLSTMCAMbiotic
from data_preprocessing2 import smiles_chars
import random
import matplotlib.pyplot as plt
from scipy.interpolate import interp1d
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit import Chem
from rdkit.Chem.Draw import DrawingOptions
import matplotlib.cm as cm
import matplotlib.colors as cc
import matplotlib.cm as cm
import matplotlib.colors as mcolors
from rdkit.Chem.Draw import SimilarityMaps
from matplotlib.colors import Normalize
from matplotlib.cm import ScalarMappable
from rdkit.Chem.Draw import rdMolDraw2D
from rdkit.Chem import rdDepictor

def smiles_to_tensor(smile, char_to_index, maxlen=350):
    """
    Convert a single SMILES string to a PyTorch tensor.
    
    Args:
    - smile (str): The SMILES string to be converted.
    - char_to_index (dict): A dictionary mapping characters to indices.
    - maxlen (int): Maximum length of the sequence to be padded to.
    
    Returns:
    - torch.Tensor: A PyTorch tensor representing the one-hot encoded SMILES.
    """
    # Convert the SMILES string to a one-hot encoded sequence
    encoded = np.zeros((maxlen, len(char_to_index)), dtype=np.int8)
    for i, char in enumerate(smile):
        if i < maxlen:  # Ensure index doesn't exceed maxlen - 1
            if char in char_to_index:
                encoded[i, char_to_index[char]] = 1
        else:
            break  # Break the loop if index equals or exceeds maxlen
    
    # Convert the numpy array to a PyTorch tensor
    encoded_tensor = torch.from_numpy(encoded).unsqueeze(0)  # Add batch dimension
    encoded_tensor = encoded_tensor.to(dtype=torch.float32)  # Model might expect float
    return encoded_tensor



class SmilesCAMVisualizer:
    def __init__(self, model, device='cpu'):
        # Load the model
        self.model = model
        self.model.eval()  # Set the model to evaluation mode
        self.model.to(device)
        self.device = device

    def prepare_input(self, smiles):
        # Assuming char_to_index is a dictionary mapping SMILES characters to integer indices
        char_to_index = {char: idx for idx, char in enumerate(smiles_chars)}
        smiles_tensor = smiles_to_tensor(smiles, char_to_index, maxlen=350).to(self.device)
        return smiles_tensor


    def forward_pass(self, smiles_tensor):
        # Forward pass through the model to get the output and feature maps
        output, _, feature_maps = self.model(smiles_tensor)
        return output, feature_maps


    def backward_pass(self, output, target_class):
        """Perform the backward pass to get the gradients for a specific class."""
        self.model.zero_grad()
        if target_class == "negative":
            # Create a tensor for the negative class
            # This assumes the model's output is a probability of the positive class
            negative_output = 1 - output
            negative_output.backward(gradient=torch.ones_like(negative_output), retain_graph=True)
            gradients = self.model.gradients['last_conv']
        else:
            # Positive class as before
            output.backward(gradient=torch.ones_like(output), retain_graph=True)
            gradients = self.model.gradients['last_conv']
        return gradients



    def generate_heatmap(self, gradients, feature_maps):

        # Generate the heatmap from the feature maps and gradients
        pooled_gradients = torch.mean(gradients, dim=[0, 2]) # average the gradients across the batch_size [0] and spatial dimensions [2] to give a 1D tensor with length equal to [1] i.e. the number of filters
        for i in range(feature_maps.size(1)):
            feature_maps[:, i, :] *= pooled_gradients[i] #for each filter in the feature maps, multiply the feature map by the corresponding pooled gradient to emphasize the important regions of the feature map for the positive class
        heatmap = torch.mean(feature_maps, dim=1).squeeze() # average the feature maps across the filters and combine them into a single heatmap
        heatmap = np.maximum(heatmap.cpu().detach().numpy(), 0) # eensures that all values in the heatmap are positive since were only interested in the regions that contribute positively to the positive class
        heatmap /= np.max(heatmap) # normalize the heatmap to the range [0, 1]
        return heatmap


   
    def interpolate_heatmap_to_atoms(self, smiles, heatmap): #Adding self to the method signature allows you to call other methods within this method and access class attributes showinng that it is an instance method
        """Interpolate the heatmap to match the number of atoms in the molecule."""
        # Convert SMILES to RDKit molecule object
        mol = Chem.MolFromSmiles(smiles)
        # Get the number of atoms
        num_atoms = mol.GetNumAtoms()

        # Create an interpolation function
        interpolate = interp1d(np.arange(len(heatmap)), heatmap, fill_value="extrapolate")
        
        # Generate new heatmap values that match the number of atoms
        new_heatmap = interpolate(np.linspace(0, len(heatmap) - 1, num_atoms))
        
        return new_heatmap    




    def overlay_heatmap_on_molecule(self, smiles, heatmap):
        # Convert SMILES to RDKit molecule object
        mol = Chem.MolFromSmiles(smiles)

        # Normalize the heatmap values to fit within the colormap range
        norm = Normalize(vmin=min(heatmap), vmax=max(heatmap))
        cmap = plt.get_cmap('Greens')  # Use Matplotlib's 'BluGrn' colormap for shades of blue and green

        # Generate the similarity map
        fig = SimilarityMaps.GetSimilarityMapFromWeights(mol, [norm(value) for value in heatmap], colorMap=cmap, size=(300, 300))

        # Show the colorbar to represent the heatmap scale
        plt.colorbar(ScalarMappable(norm=norm, cmap=cmap), ax=fig.axes[0])

        plt.show()


    def visualize_molecule_with_heatmap(self, smiles, target_class="positive"): #change to "negative" to visualize the heatmap for the negative class
        # Convert SMILES to input tensor
        input_tensor = self.prepare_input(smiles)
        
        # Generate heatmap
        output, feature_maps = self.forward_pass(input_tensor)
        gradients = self.backward_pass(output, target_class=target_class)
        original_heatmap = self.generate_heatmap(gradients, feature_maps)
        
        # Interpolate heatmap to match the number of atoms
        interpolated_heatmap = self.interpolate_heatmap_to_atoms(smiles, original_heatmap)
        
        # Overlay heatmap on molecule and visualize
        self.overlay_heatmap_on_molecule(smiles, interpolated_heatmap)

    



maxlen = 350 # change to your maxlen
#model_checkpoint = 'checkpoint-epoch=194-val_loss=0.48.ckpt' # change to your saved model
trained_model = ConvLSTMCAMbiotic.load_from_checkpoint(model_checkpoint)
visualizer = SmilesCAMVisualizer(trained_model, device='cpu')  # object of SmilesCAMVisualizer class created with the trained model
#smiles = 'CNC1CCN(c2nc3c(cc2F)c(=O)c(C(=O)O)cn3CCF)C1' # ACTIVE molecule example
smiles = 'Cc1ccc(NC(=S)NC(=O)c2cnn(C)c2)cc1' # INACTIVE molecule example
visualizer.visualize_molecule_with_heatmap(smiles, target_class="positive") # visualize the molecule with the heatmap using the trained model, change to "negative" to visualize the heatmap for the negative class


